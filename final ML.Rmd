---
title: "Analyzing and Predicting Forest Fires in the Montesinho Natural Park"
author: "Group 5: Zac Macintyre and Charlotte Wang"
date: "6/21/2021"
output: pdf_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(boot)
library(mltools)
library(data.table)
library(psych)
library(glmnet)
library(tidyverse)
library(caret)
library(leaps)
library(factoextra)
library(cluster)
fires = read.csv("ForestFires.csv")
head(fires)
```

```{r}
colSums(is.na(fires))
```
As we said in the presentation there is no missing values just doing a little check

# This section is some initial data visualization and some data analysis

As suggested in Cortez and Morais (2007), transforming area to log(area+1) can correct area's skewness toward zero

```{r}
log_fires = fires
log_fires$log_area = log(log_fires$area +1)
log_fires = log_fires[, -c(13)]
```

```{r}
barplot(table(fires$month), ylab = "Frequency", main = "Frequency of Forest Fires per Month")
barplot(table(fires$day))
```
```{r}
hist(log_fires$log_area, xlab = "Log Area of Damage (Hecters)", main = "Histogram of Log Area Damage")
```


```{r}
summary(fires)
```

```{r}
par(mfrow=c(2,2))
plot(fires$rain, fires$area)
plot(fires$temp, fires$area)
plot(fires$FFMC, fires$area)
plot(fires$DMC, fires$area)
```
```{r}
par(mfrow=c(2,2))
plot(fires$DC, fires$area)
plot(fires$ISI, fires$area)
plot(fires$wind, fires$area)
plot(fires$RH, fires$area)
```

Some summary statistics
```{r}
summary(fires)
describe(fires, skew = FALSE, omit = TRUE)
table(fires$month)
table(fires$day)
```

Correlation
```{r}
fires_matirx = fires[,-c(3,4)]
corelation_m = cor(fires_matirx, use='pairwise.complete.obs')
lower = lower.tri(fires_matirx)
corelation_m
hist(corelation_m[lower], xlab = 'correlations of lower matrix')
```

This is for fires not using X,Y,Day,Month
```{r}
fires_lm = log_fires[,-c(1,2,3,4)]
#head(log_fires_lm)
#head(fires_lm)
#fitting the most generic model
set.seed(1)
test = sample(nrow(fires_lm), floor(nrow(fires_lm)*.2))
linear_fit = lm(log_area~., data = fires_lm[-test,])
summary(linear_fit)
# nothing significant

# getting MSE for the test
pred = predict(linear_fit, fires_lm[test,])
mean((pred - fires_lm[test, "log_area"])**2)

linear_fit = glm(log_area ~., data = fires_lm[-test,])
summary(linear_fit)

#10 CV validation for the training set 
cv.glm(fires_lm[-test,], linear_fit, K = 10)$delta[1]

# getting MSE for the test
pred = predict(linear_fit, fires_lm[test,])
mean((pred - fires_lm[test, "log_area"])**2)
```

This is for fires not using Day,Month
```{r}
fires_lm = log_fires[,-c(3,4)]
#head(fires_lm)
#fitting the most generic model
fires_lm = log_fires[,-c(3,4)]
#head(fires_lm)
#fitting the most generic model
linear_fit = lm(log_area~., data = fires_lm[-test,])
linear_fit = glm(log_area ~ ., data = fires_lm[-test,])
summary(linear_fit) # wind significant

#10 CV validation for the training set 
cv.glm(fires_lm[-test,], linear_fit, K = 10)$delta[1]

# getting MSE for the test
pred = predict(linear_fit, fires_lm[test,])
mean((pred - fires_lm[test, "log_area"])**2)
```

```{r}
log_fires$month = as.factor(log_fires$month)
log_fires$day = as.factor(log_fires$day)

newdata = one_hot(as.data.table(log_fires))
newdata
```

```{r}
linear_fit = glm(log_area~., data = newdata[-test,])
summary(linear_fit) # month_dec significant, but only 9 instances

#10 CV validation for the training set 
cv.glm(fires_lm[-test,], linear_fit, K = 10)$delta[1]

# getting MSE for the test
pred = predict(linear_fit, newdata[test,])
mean((pred - fires_lm[test, "log_area"])**2)
```

Trying different sub-selection models now
```{r}
# Set seed for reproducibility
set.seed(123)
# Set up repeated k-fold cross-validation
train.control <- trainControl(method = "cv", number = 10)
# Train the model
step.model <- train(log_area ~., data = newdata[-test,],
                    method = "leapForward", 
                    tuneGrid = data.frame(nvmax = 1:28),
                    trControl = train.control
                    )
step.model$results

step.model$bestTune
summary(step.model$finalModel)
```

Using the above forward selection method we got that temp was the best forward selection model.  Here is code of us actually checking the MSE for temp
```{r}
linear_fit = glm(log_area~month_dec, data = newdata[-test,])

# getting MSE for the test
pred = predict(linear_fit, newdata[test,])
mean((pred - fires_lm[test, "log_area"])**2)
```


```{r}
# Set seed for reproducibility
set.seed(123)
# Set up repeated k-fold cross-validation
train.control <- trainControl(method = "cv", number = 10)
# Train the model
step.model <- train(log_area ~., data = newdata[-test,],
                    method = "leapBackward", 
                    tuneGrid = data.frame(nvmax = 1:28),
                    trControl = train.control
                    )
step.model$results

step.model$bestTune
summary(step.model$finalModel)
```

Backwards selection choose the model with 1 variables - month dec
```{r}
linear_fit = glm(log_area~month_dec, data = newdata[-test,])
summary(linear_fit)

# getting MSE for the test
pred = predict(linear_fit, newdata[test,])
mean((pred - fires_lm[test, "log_area"])**2)
```

```{r}
# Set seed for reproducibility
set.seed(123)
# Set up repeated k-fold cross-validation
train.control <- trainControl(method = "cv", number = 10)
# Train the model
step.model <- train(log_area ~., data = newdata[-test,],
                    method = "leapSeq", 
                    tuneGrid = data.frame(nvmax = 1:20),
                    trControl = train.control
                    )
step.model$results

step.model$bestTune
summary(step.model$finalModel)
```
We get out the 2 variable model with month_dec and temp, just like forward selection. 


I would like to explore some interaction terms and maybe investigating more into the correlated variables 
```{r}
summary(lm(log_area ~ temp*day_sat, newdata))
```

# Lasso
```{r}
grid=10^seq(10,-2,length=100)
lasso_mod = glmnet(x = as.matrix(newdata[-test, -"log_area"]), y = as.matrix(newdata[-test, log_area]), alpha=1, lambda = grid)
plot(lasso_mod)

# cross validation
cv_lasso = cv.glmnet(x = as.matrix(newdata[-test, -"log_area"]), y = as.matrix(newdata[-test, log_area]), alpha=1)
plot(cv_lasso)
bestlam = cv_lasso$lambda.min
lasso_pred=predict(lasso_mod,s=bestlam, newx=as.matrix(newdata[-test, -"log_area"]))
mean((lasso_pred-as.matrix(newdata[-test, log_area]))^2 - as.matrix(newdata[-test, log_area]))

out=glmnet(as.matrix(newdata[, -"log_area"]), as.matrix(newdata[, log_area]), alpha=1,lambda=grid)
lasso_coef=predict(out,type="coefficients",s=bestlam)
lasso_coef
```

# try different coding method
As described in hw2 we can combine days to weekdays & weekend
```{r}
newcodingdat <- log_fires
newcodingdat$weekend <- ifelse(newcodingdat$day == "sat" | 
                              newcodingdat$day == "sun", 1, 0)
fires_lm = newcodingdat[,-c(3,4)]
#head(fires_lm)
#fitting the most generic model
linear_fit = lm(log_area~., data = fires_lm[-test,])
summary(linear_fit)

linear_fit = glm(log_area ~ ., data = fires_lm[-test,])

#10 CV validation for the training set 
cv.glm(fires_lm[-test,], linear_fit, K = 10)$delta[1]

# getting MSE for the test
pred = predict(linear_fit, fires_lm[test,])
mean((pred - fires_lm[test, "log_area"])**2)
```

coding seasons too
```{r}
newcodingdat$season <- newcodingdat$month
newcodingdat$season<- recode(newcodingdat$season, jan = "winter", feb = "winter",
       mar = "spring", apr = "spring", may = "spring",
       jun = "summer", jul = "summer", aug = "summer",
       sep = "fall", oct = "fall", nov = "fall",
       dec = "winter")
newdata_2 = one_hot(as.data.table(newcodingdat[, -c(3,4)]))
fires_lm = newdata_2
linear_fit = lm(log_area~., data = newdata_2[-test,])
summary(linear_fit) # this one seem to be better(?) 

linear_fit = glm(log_area ~ ., data = fires_lm[-test,])

#10 CV validation for the training set 
cv.glm(fires_lm[-test,], linear_fit, K = 10)$delta[1]

# getting MSE for the test
pred = predict(linear_fit, fires_lm[test,])
sum((pred - fires_lm[test, "log_area"])**2)/103
mean((pred - fires_lm[test, "log_area"])**2)
```

Lasso after new coding

```{r}
grid=10^seq(10,-2,length=100)
lasso_mod = glmnet(x = as.matrix(newdata_2[-test, -"log_area"]), y = as.matrix(newdata_2[-test, log_area]), alpha=1, lambda = grid)
plot(lasso_mod)

# cross validation
cv_lasso = cv.glmnet(x = as.matrix(newdata_2[-test, -"log_area"]), y = as.matrix(newdata_2[-test, log_area]), alpha=1)
plot(cv_lasso)
bestlam = cv_lasso$lambda.min
lasso_pred=predict(lasso_mod,s=bestlam, newx=as.matrix(newdata_2[-test, -"log_area"]))
mean((lasso_pred-as.matrix(newdata_2[-test, log_area]))^2 - as.matrix(newdata_2[-test, log_area]))

out=glmnet(as.matrix(newdata_2[, -"log_area"]), as.matrix(newdata_2[, log_area]), alpha=1,lambda=grid)
lasso_coef=predict(out,type="coefficients",s=bestlam)
lasso_coef
```


# Classification
```{r}
median(log_fires$log_area) #values below this are considered small fires
bi_fires <- log_fires
bi_fires$large_fire <- ifelse(bi_fires$log_area < median(log_fires$log_area), 0, 1)
fires_lm <- bi_fires[, -c(1,2,3,4,13)]

# Logit
set.seed(1)
test = sample(nrow(fires_lm), floor(nrow(fires_lm)*.2))

# train the model on training set
train_control <- trainControl(method = "cv", number = 10)

logit_fit <- train(as.factor(large_fire) ~ .,
               data = fires_lm[-test, ],
               trControl = train_control,
               method = "glm",
               family=binomial())
summary(logit_fit)

prob = predict(logit_fit, fires_lm[test,], type = "prob")
pred=rep(0, 517)
pred[prob >0.5]=1
table(pred,as.factor(fires_lm$large_fire))

```
Adding location variable

```{r}

fires_lm <- bi_fires[, -c(3,4,13)]

# Logit
set.seed(1)
test = sample(nrow(fires_lm), floor(nrow(fires_lm)*.2))

# train the model on training set
train_control <- trainControl(method = "cv", number = 10)

logit_fit <- train(as.factor(large_fire) ~ .,
               data = fires_lm[-test, ],
               trControl = train_control,
               method = "glm",
               family=binomial())
summary(logit_fit)

prob = predict(logit_fit, fires_lm[test,], type = "prob")
pred=rep(0, 517)
pred[prob >0.5]=1
table(pred,as.factor(fires_lm$large_fire))

```
All variables

```{r}
fires_lm <- one_hot(as.data.table(bi_fires))
fires_lm <- fires_lm[, -c(30)]
# Logit
set.seed(1)
test = sample(nrow(fires_lm), floor(nrow(fires_lm)*.2))

# train the model on training set
train_control <- trainControl(method = "cv", number = 10)

logit_fit <- train(as.factor(large_fire) ~ .,
               data = fires_lm[-test, ],
               trControl = train_control,
               method = "glm",
               family=binomial())
summary(logit_fit)

prob = predict(logit_fit, fires_lm[test,], type = "prob")
pred=rep(0, 517)
pred[prob >0.5]=1
table(pred,as.factor(fires_lm$large_fire))


```

Using weekday/weekend and seasons
```{r}
fires_lm = newdata_2
fires_lm$large_fire = ifelse(fires_lm$log_area < median(fires_lm$log_area), 0, 1)
fires_lm = fires_lm[, -c(16)]

set.seed(1)
test = sample(nrow(fires_lm), floor(nrow(fires_lm)*.2))

# train the model on training set
train_control <- trainControl(method = "cv", number = 10)

logit_fit <- train(as.factor(large_fire) ~ .,
               data = fires_lm[-test, ],
               trControl = train_control,
               method = "glm",
               family=binomial())
summary(logit_fit)

prob = predict(logit_fit, fires_lm[test,], type = "prob")
pred=rep(0, 517)
pred[prob >0.5]=1
table(pred,as.factor(fires_lm$large_fire))


```


# Clustering methods

k-medoid clustering
```{r}
pam(newdata, 2, metric = "euclidean", stand = TRUE)
fviz_nbclust(newdata, pam, method = "wss")
# look at elbow --> three clusters

# another approach
gap_stat <- clusGap(newdata, FUN = pam, K.max = 15,B = 50)
fviz_gap_stat(gap_stat)
# k =14 -> no, overfit

# choosing 3 clusters seem fine?
set.seed(1)
kmed <- pam(newdata, k = 3)
kmed
fviz_cluster(kmed, data = newdata)

# combine clusters into the dataset
labeled_data = cbind(newdata, cluster = kmed$cluster)
head(labeled_data)

summary(lm(log_area ~ as.factor(cluster), data = labeled_data))
# significant....

# Look into these clusters
mod_cl1 <-lm(log_area ~ ., data = labeled_data[cluster == 1,-c(31)])
summary(mod_cl1)
mod_cl2 <-lm(log_area ~ ., data = labeled_data[cluster == 2,-c(31)])
summary(mod_cl2)
mod_cl3 <-lm(log_area ~ ., data = labeled_data[cluster == 3,-c(31)])
summary(mod_cl3)
```

Try without qualitative variables?

```{r}
clusterdat <- as.data.frame(log_fires[, -c(3,4)])
pam(clusterdat, 2, metric = "euclidean", stand = TRUE)
fviz_nbclust(clusterdat, pam, method = "wss")
# look at elbow --> three clusters

# another approach
gap_stat <- clusGap(clusterdat, FUN = pam, K.max = 15,B = 50)
fviz_gap_stat(gap_stat)

# choosing 3 clusters seem fine?
set.seed(1)
kmed <- pam(clusterdat, k = 3)
kmed
fviz_cluster(kmed, data = clusterdat)

# combine clusters into the dataset
labeled_data = cbind(clusterdat, cluster = kmed$cluster)
head(labeled_data)

summary(lm(log_area ~ as.factor(cluster), data = labeled_data))
# significant....

# Look into these clusters
mod_cl1 <-lm(log_area ~ ., data = labeled_data %>% 
               filter(cluster == 1) %>%
               select(-c(12)))
summary(mod_cl1)
mod_cl2 <-lm(log_area ~ ., data = labeled_data %>% 
               filter(cluster == 2) %>%
               select(-c(12)))
summary(mod_cl2)
mod_cl3 <-lm(log_area ~ ., data = labeled_data %>% 
               filter(cluster == 3) %>%
               select(-c(12)))
summary(mod_cl3)
# Not as informative
```


Hierarchical clustering

```{r}
dist_mat <- dist(newdata, method = 'euclidean')
hclust_avg <- hclust(dist_mat, method = 'average')
plot(hclust_avg)
cut <- cutree(hclust_avg, k = 3)
labeled_data = cbind(newdata, cluster = cut)
summary(lm(log_area ~ as.factor(cluster), data = labeled_data))


```
