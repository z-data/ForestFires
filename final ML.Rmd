---
title: "final ML"
author: "Zac Macintyre"
date: "6/9/2021"
output: pdf_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(boot)
library(mltools)
library(data.table)
library(psych)
library(glmnet)
library(tidyverse)
library(caret)
library(leaps)
library(factoextra)
library(cluster)
fires = read.csv("ForestFires.csv")
head(fires)
```

```{r}
colSums(is.na(fires))
```
As we said in the presentation there is no missing values just doing a little check

# This section is me doing some initial data visualization and some data analysis

As suggested in Cortez and Morais (2007), transforming area to log(area+1) can correct area's skewness toward zero
```{r}
log_fires = fires
log_fires$log_area = log(log_fires$area +1)
log_fires = log_fires[, -c(13)]
```

# This section is me doing some initial data visualization and somre data analysis
```{r}
barplot(table(fires$month))
barplot(table(fires$day))
```
```{r}
hist(log_fires$log_area)
```


```{r}
summary(fires)
```

```{r}
par(mfrow=c(2,2))
plot(fires$rain, fires$area)
plot(fires$temp, fires$area)
plot(fires$FFMC, fires$area)
plot(fires$DMC, fires$area)
```
```{r}
par(mfrow=c(2,2))
plot(fires$DC, fires$area)
plot(fires$ISI, fires$area)
plot(fires$wind, fires$area)
plot(fires$RH, fires$area)
```

Some summary statistics
```{r}
summary(fires)
describe(fires)
```

Correlation
```{r}
fires_matirx = fires[,-c(3,4)]
corelation_m = cor(fires_matirx, use='pairwise.complete.obs')
lower = lower.tri(fires_matirx)
corelation_m
hist(corelation_m[lower], xlab = 'correlations of lower matrix')
```



This is for fires not using X,Y,Day,Month
```{r}
fires_lm = log_fires[,-c(1,2,3,4)]
#head(log_fires_lm)
#head(fires_lm)
#fitting the most generic model
set.seed(1)
test = sample(nrow(fires_lm), floor(nrow(fires_lm)*.2))
linear_fit = lm(log_area~., data = fires_lm[-test,])
summary(linear_fit)
# nothing significant

# getting MSE for the test
pred = predict(linear_fit, fires_lm[test,])
mean((pred - fires_lm[test, "log_area"])**2)

linear_fit = glm(log_area ~., data = fires_lm[-test,])
summary(linear_fit)

#10 CV validation for the training set 
cv.glm(fires_lm[-test,], linear_fit, K = 10)$delta[1]

# getting MSE for the test
pred = predict(linear_fit, fires_lm[test,])
mean((pred - fires_lm[test, "log_area"])**2)
```


This is for fires not using Day,Month
```{r}
fires_lm = log_fires[,-c(3,4)]
#head(fires_lm)
#fitting the most generic model
test = sample(nrow(fires_lm), floor(nrow(fires_lm)*.2))
linear_fit = lm(log_area~., data = fires_lm[-test,])
linear_fit = glm(log_area ~ ., data = fires_lm[-test,])
summary(linear_fit) # wind significant

#10 CV validation for the training set 
cv.glm(fires_lm[-test,], linear_fit, K = 10)$delta[1]

# getting MSE for the test
pred = predict(linear_fit, fires_lm[test,])
mean((pred - fires_lm[test, "log_area"])**2)
```


```{r}
log_fires$month = as.factor(log_fires$month)
log_fires$day = as.factor(log_fires$day)

newdata = one_hot(as.data.table(log_fires))
newdata
```

```{r}
test = sample(nrow(fires_lm), floor(nrow(fires_lm)*.2))
linear_fit = glm(log_area~., data = newdata[-test,])
summary(linear_fit) # month_dec significant, but only 9 instances

#10 CV validation for the training set 
cv.glm(fires_lm[-test,], linear_fit, K = 10)$delta[1]

# getting MSE for the test
pred = predict(linear_fit, newdata[test,])
mean((pred - fires_lm[test, "log_area"])**2)
```

Trying different sub-selection models now
```{r}
# Set seed for reproducibility
set.seed(123)
# Set up repeated k-fold cross-validation
train.control <- trainControl(method = "cv", number = 10)
# Train the model
step.model <- train(log_area ~., data = newdata[-test,],
                    method = "leapForward", 
                    tuneGrid = data.frame(nvmax = 1:28),
                    trControl = train.control
                    )
step.model$results

step.model$bestTune
summary(step.model$finalModel)
```

Using the above forward selection method we got that temp was the best forward selection model.  Here is code of us actually checking the MSE for temp
```{r}
linear_fit = glm(log_area~month_dec, data = newdata[-test,])

# getting MSE for the test
pred = predict(linear_fit, newdata[test,])
mean((pred - fires_lm[test, "log_area"])**2)
```


```{r}
# Set seed for reproducibility
set.seed(123)
# Set up repeated k-fold cross-validation
train.control <- trainControl(method = "cv", number = 10)
# Train the model
step.model <- train(log_area ~., data = newdata[-test,],
                    method = "leapBackward", 
                    tuneGrid = data.frame(nvmax = 1:28),
                    trControl = train.control
                    )
step.model$results

step.model$bestTune
summary(step.model$finalModel)
```

Backwards selection choose the model with 11 variables 
```{r}

linear_fit = glm(log_area~X+month_aug+month_dec+month_jul+month_jun+month_mar+DMC+temp+wind, data = newdata[-test,])
summary(linear_fit)

# getting MSE for the test
pred = predict(linear_fit, newdata[test,])
mean((pred - fires_lm[test, "log_area"])**2)
```

```{r}
# Set seed for reproducibility
set.seed(123)
# Set up repeated k-fold cross-validation
train.control <- trainControl(method = "cv", number = 10)
# Train the model
step.model <- train(log_area ~., data = newdata[-test,],
                    method = "leapSeq", 
                    tuneGrid = data.frame(nvmax = 1:20),
                    trControl = train.control
                    )
step.model$results

step.model$bestTune
summary(step.model$finalModel)
```
Still getting just the one variable model with only month_dec 

I would like to explore some interaction terms and maybe investigating more into the correlated variables 
```{r}
summary(lm(log_area ~ temp*day_sat, newdata))
```



# Lasso
```{r}
grid=10^seq(10,-2,length=100)
lasso_mod = glmnet(x = as.matrix(newdata[-test, -"log_area"]), y = as.matrix(newdata[-test, log_area]), alpha=1, lambda = grid)
plot(lasso_mod)

# cross validation
cv_lasso = cv.glmnet(x = as.matrix(newdata[-test, -"log_area"]), y = as.matrix(newdata[-test, log_area]), alpha=1)
plot(cv_lasso)
bestlam = cv_lasso$lambda.min
lasso_pred=predict(lasso_mod,s=bestlam, newx=as.matrix(newdata[-test, -"log_area"]))
mean((lasso_pred-as.matrix(newdata[-test, log_area]))^2 - as.matrix(newdata[-test, log_area]))

out=glmnet(as.matrix(newdata[, -"log_area"]), as.matrix(newdata[, log_area]), alpha=1,lambda=grid)
lasso_coef=predict(out,type="coefficients",s=bestlam)
lasso_coef
```

# try different coding method
As described in hw2 we can combine days to weekdays & weekend
```{r}
newcodingdat <- log_fires
newcodingdat$weekend <- ifelse(newcodingdat$day == "sat" | 
                              newcodingdat$day == "sun", 1, 0)
fires_lm = newcodingdat[,-c(3,4)]
#head(fires_lm)
#fitting the most generic model
test = sample(nrow(fires_lm), floor(nrow(fires_lm)*.2))
linear_fit = lm(log_area~., data = fires_lm[-test,])
summary(linear_fit)

linear_fit = glm(log_area ~ ., data = fires_lm[-test,])

#10 CV validation for the training set 
cv.glm(fires_lm[-test,], linear_fit, K = 10)$delta[1]

# getting MSE for the test
pred = predict(linear_fit, fires_lm[test,])
mean((pred - fires_lm[test, "log_area"])**2)
```

coding seasons too
```{r}
newcodingdat$season <- newcodingdat$month
newcodingdat$season<- recode(newcodingdat$season, jan = "winter", feb = "winter",
       mar = "spring", apr = "spring", may = "spring",
       jun = "summer", jul = "summer", aug = "summer",
       sep = "fall", oct = "fall", nov = "fall",
       dec = "winter")
newdata_2 = one_hot(as.data.table(newcodingdat[, -c(3,4)]))
fires_lm = newdata_2
test = sample(nrow(fires_lm), floor(nrow(fires_lm)*.2))
linear_fit = lm(log_area~., data = newdata_2[-test,])
summary(linear_fit) # this one seem to be better(?) 

linear_fit = glm(log_area ~ ., data = fires_lm[-test,])

#10 CV validation for the training set 
cv.glm(fires_lm[-test,], linear_fit, K = 10)$delta[1]

# getting MSE for the test
pred = predict(linear_fit, fires_lm[test,])
mean((pred - fires_lm[test, "log_area"])**2)
```

Lasso after new coding

```{r}
grid=10^seq(10,-2,length=100)
lasso_mod = glmnet(x = as.matrix(newdata_2[-test, -"log_area"]), y = as.matrix(newdata_2[-test, log_area]), alpha=1, lambda = grid)
plot(lasso_mod)

# cross validation
cv_lasso = cv.glmnet(x = as.matrix(newdata_2[-test, -"log_area"]), y = as.matrix(newdata_2[-test, log_area]), alpha=1)
plot(cv_lasso)
bestlam = cv_lasso$lambda.min
lasso_pred=predict(lasso_mod,s=bestlam, newx=as.matrix(newdata_2[-test, -"log_area"]))
mean((lasso_pred-as.matrix(newdata_2[-test, log_area]))^2 - as.matrix(newdata_2[-test, log_area]))

out=glmnet(as.matrix(newdata_2[, -"log_area"]), as.matrix(newdata_2[, log_area]), alpha=1,lambda=grid)
lasso_coef=predict(out,type="coefficients",s=bestlam)
lasso_coef
```

# Clustering methods

k-medoid clustering
```{r}
pam(newdata_2, 2, metric = "euclidean", stand = TRUE)
fviz_nbclust(newdata_2, pam, method = "wss")

gap_stat <- clusGap(newdata_2, FUN = pam, K.max = 5,B = 50)
fviz_gap_stat(gap_stat)

# choosing 2 clusters seem fine?
set.seed(1)
kmed <- pam(newdata_2, k = 2)
kmed
fviz_cluster(kmed, data = newdata_2)

# seem to be three clusters? redo with three clusters.
pam(newdata_2, 2, metric = "euclidean", stand = TRUE)
set.seed(1)
kmed <- pam(newdata, k = 2)
kmed
fviz_cluster(kmed, newdata_2)

# combine clusters into the dataset
labeled_data = cbind(newdata_2, cluster = kmed$cluster)
head(labeled_data)

summary(lm(log_area ~ as.factor(cluster), data = labeled_data))
# not significant....
```

Hierarchical clustering

```{r}
dist_mat <- dist(newdata_2, method = 'euclidean')
hclust_avg <- hclust(dist_mat, method = 'average')
plot(hclust_avg)
cut <- cutree(hclust_avg, k = 3)
labeled_data = cbind(newdata_2, cluster = cut)
summary(lm(log_area ~ as.factor(cluster), data = labeled_data))

# again not very significant. What about k=5? 
cut <- cutree(hclust_avg, k = 5)
labeled_data = cbind(newdata_2, cluster = cut)
summary(lm(log_area ~ as.factor(cluster), data = labeled_data))
# it's worse
```
